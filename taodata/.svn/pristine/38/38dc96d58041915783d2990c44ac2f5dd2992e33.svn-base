#!/usr/bin/env python
# encoding: utf-8
import re
from scrapy.crawler import CrawlerProcess
from scrapy.selector import Selector
from scrapy.http import Request
from scrapy.utils.project import get_project_settings
from scrapy_redis.spiders import RedisSpider
import sys
import os
sys.path.append(os.getcwd())
from taodata.settings import LOCAL_REDIS_HOST, LOCAL_REDIS_PORT, LOCAL_REDIS_PASSWORD
import time
import json
import redis
import datetime
import uuid
from taodata.spiders.cues import cues_util
from twisted.internet.error import TimeoutError, TCPTimedOutError, ConnectionRefusedError
from twisted.web._newclient import ResponseFailed, ResponseNeverReceived
from scrapy.spidermiddlewares.httperror import HttpError
from scrapy.utils.response import response_status_message  # 获取错误代码信息
import traceback


class WbHotBlogSpider(RedisSpider):
    r = redis.Redis(host=LOCAL_REDIS_HOST, port=LOCAL_REDIS_PORT, password=LOCAL_REDIS_PASSWORD)
    name = "cues:crawl:wb_hot_blog"
    redis_key = "cues:crawl:wb_hot_blog:start_urls"

    custom_settings = {
        'CONCURRENT_REQUESTS': 4,
        "DOWNLOAD_DELAY": 0.5,
        'DOWNLOAD_TIMEOUT': 30,
        "SCHEDULER": "scrapy_redis.scheduler.Scheduler",
        "DUPEFILTER_CLASS": "scrapy_redis.dupefilter.RFPDupeFilter",
        "SCHEDULER_PERSIST": True,
        'DEFAULT_REQUEST_HEADERS': {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'
        },
        'DOWNLOADER_MIDDLEWARES': {
            'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': None,
            'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': None,
            'scrapy.downloadermiddlewares.retry.RetryMiddleware': None,
            'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
            'taodata.middlewares.RetryMiddleware': 550,
            'taodata.middlewares.HotWeiboCookieMiddleware': 300,
            'taodata.middlewares.RedirectMiddleware': 200,
            # 'taodata.middlewares.IPPoolMiddleware': 125,
            'taodata.middlewares.ProxyMiddleware': 125,
            #'taodata.middlewares.NoProxyMiddleware': 125,
        },
        'ITEM_PIPELINES': {
            'taodata.spiders.cues.pipelines.CuesMongoDBPipeline': 999
        },
        'RETRY_TIMES': 50,
        'RETRY_ENABLED': True
    }

    # 默认初始解析函数
    def parse(self, response):
        url = response.url
        container_id = cues_util.parse_query(url, 'containerid')
        container_id_times = self.r.hget('cues:crawl:temp:wb_hot_blog', container_id + '_times')
        if container_id_times and int(container_id_times) >= 50:
            self.r.hdel('cues:crawl:temp:wb_hot_blog', container_id)
            self.r.hdel('cues:crawl:temp:wb_hot_blog', container_id + '_times')

        text1 = response.text
        try:
            json1 = json.loads(text1)
            if 'ok' in json1 and json1['ok'] != 1:
                self.r.hdel('cues:crawl:temp:wb_hot_blog', container_id)
                self.r.hdel('cues:crawl:temp:wb_hot_blog', container_id+'_times')
                self.r.sadd('cues:crawl:wb:error', response.url)
            else:
                if 'data' in json1 and 'cards' in json1['data']:
                    cards = json1['data']['cards']
                    print(len(cards))
                    for card in cards:
                        if 'mblog' in card:
                            mblog = card['mblog']
                            prefix = datetime.datetime.now().strftime('%Y%m%d')
                            if self.r.sismember('cues:crawl:dupe:wb_base_blog:'+prefix, mblog['id']):
                                pass
                            else:
                                if 'user' in mblog:
                                    user = mblog['user']
                                    if self.r.sismember('cues:crawl:dupe:wb_base_profile', user['id']):
                                        pass
                                    else:
                                        item = cues_util.build_wb_base_profile_item(user)
                                        yield item
                                        self.r.sadd('cues:crawl:dupe:wb_base_profile', user['id'])
                                        print('new user-->'+str(user['id']))
                                item = cues_util.build_wb_base_blog_item(mblog, 'hot_blog')
                                yield item
                                self.r.sadd('cues:crawl:dupe:wb_base_blog:'+prefix, mblog['id'])
                                print('new blog-->'+str(mblog['id']))
                        else:
                            self.r.sadd('cues:crawl:wb:error', response.url)
        except:
            traceback.print_exc()
            self.r.sadd('cues:crawl:wb:error', response.url)


if __name__ == "__main__":
    process = CrawlerProcess(get_project_settings())
    process.crawl(WbHotBlogSpider)
    process.start()
