#!/usr/bin/env python
# encoding: utf-8
import re
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
import sys
import os
sys.path.append(os.getcwd())
from multiprocessing import Process, Lock
from taodata.spiders.cues.wb_user_base_blog_dialy_spider import WbUserBaseBlogDailySpider


def crawl(lock, num):
    lock.acquire()
    print('pid'+str(os.getpid())+'***crawl'+str(num)+'->start')
    lock.release()
    process = CrawlerProcess(get_project_settings())
    process.crawl(WbUserBaseBlogDailySpider)
    process.start()


if __name__ == "__main__":
    lock = Lock()  # 这个一定要定义为全局
    for num in range(8):
        Process(target=crawl, args=(lock, num)).start()
